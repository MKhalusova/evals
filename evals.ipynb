{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Selecting an embedding model for your custom data",
   "id": "fb98851653cfa29b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In a recent blog post, [\"Understanding embedding models: make an informed choice for your RAG\"](https://unstructured.io/blog/understanding-embedding-models-make-an-informed-choice-for-your-rag), we have explored what you need to know in order to navigate the [Hugging Face MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) effortlessly and select a baseline text embedding model. \n",
    "\n",
    "You are likely to find more than one candidate model that meets your criteria. In this case, you should evaluate the candidates on your own data. Good performance on academic benchmarks is one thing, but your custom data has its own nuances, domain-specific language, and other unique traits. In this example  we'll build an end-to-end data processing pipeline with Unstructured Serverless API, and will show how to incorporate a model evaluation step into it.   \n",
    "\n",
    "We'll be comparing the performance of three embedding models from the MTEB leaderboard: [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5), [mukaj/fin-mpnet-base](https://huggingface.co/mukaj/fin-mpnet-base), and [Snowflake/snowflake-arctic-embed-l](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) on financial data, specifically, annual reports (Form-10K) that are standard in the US. \n",
    "\n",
    "This notebook covers:\n",
    "* Preprocessing data from PDFs\n",
    "* Generating a synthetic question-answer dataset\n",
    "* Measuring models' performance\n",
    "* Completing the data preprocessing pipeline with the embedding step using the best model"
   ],
   "id": "a11c8e4a60143ecf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install the required dependencies\n",
    "\n",
    "In this notebook, we'll be using a local `Llama3.1:8b` model via Ollama to generate a synthetic dataset. \n",
    "Go to https://ollama.com and download the app for your OS, then pull the model onto your local machine.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "```\n",
    "\n",
    "Alternatively, we show how to use Claude Sonnet 3.5. To switch to Anthropic, acquire your key, and add `anthropic` to pip installs. \n",
    "\n",
    "Next, let's install the libraries that we will be using: \n",
    "\n",
    "* `unstructured` & `unstructured-ingest` for preprocessing documents. \n",
    "* `python-dotenv` to load the environment variables from a `.env` file \n",
    "* `chromadb` and `langchain` to set up retrievers with different embedding models\n",
    "* `ollama` to prompt an LLM to generate a synthetic evaluation dataset\n",
    "\n",
    "\n",
    "To use this example, you'll need to get an [Unstructured API key](https://unstructured.io/api-key-hosted). The Unstructured Serverless API comes with a 14-day trial capped at 1000 pages per day. "
   ],
   "id": "4880c94377d80ec7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install -qU \"unstructured-ingest[pdf, embed-huggingface]\" unstructured python-dotenv langchain chromadb ollama",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the environment variables",
   "id": "de47b15154d698cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Store your environment variables, such as `UNSTRUCTURED_API_KEY` and `UNSTRUCTURED_URL` in a `.env` file, then load them here. ",
   "id": "956ca69e6e8aa491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:48:16.118136Z",
     "start_time": "2024-08-20T14:48:16.113086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('.env')"
   ],
   "id": "e8b7e875f10eceec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess PDFs from a source location\n",
    "\n",
    "The data we use in this example is stored in 2 large PDF files (feel free to substitute with your own data). These are annual financial reports (Form 10-K) for the year 2023 from two large companies - Walmart Inc., and Exxon Mobil Corporation. These documents are publicly available on these companies' respective websites. We'll use them as an example of domain-specific data (financial industry). \n",
    "\n",
    "As we don't have the actual user queries to evaluate retrieval performance, the next best thing is to generate an evaluation dataset from custom data. For this, we will first need to preprocess the PDFs. First, let's do the necessary imports: "
   ],
   "id": "356e8798d2b085ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:47:19.079047Z",
     "start_time": "2024-08-20T14:47:19.074537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import ollama\n",
    "# import anthropic\n",
    "import pandas as pd\n",
    "\n",
    "from unstructured_ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured_ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured_ingest.v2.processes.connectors.local import (\n",
    "    LocalIndexerConfig,\n",
    "    LocalDownloaderConfig,\n",
    "    LocalConnectionConfig,\n",
    "    LocalUploaderConfig\n",
    ")\n",
    "from unstructured_ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured_ingest.v2.processes.chunker import ChunkerConfig\n",
    "from unstructured_ingest.v2.processes.embedder import EmbedderConfig\n",
    "\n",
    "from unstructured.staging.base import elements_from_json\n",
    "from unstructured.staging.base import elements_to_dicts\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.vectorstores import utils as chromautils\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ],
   "id": "b5e20edbe453e280",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To process the PDFs from a local directory, set up Unstructured ingest pipeline with a local source connector, and a local destination connector. Unstructured supports dozens of source and destination connectors, so you can easily modify this pipeline to ingest documents from an S3 bucket, or Azure blob storage, or Google Drive, or any of the other supported sources. \n",
    "At this stage, let's keep the destination local. Once we're done evaluating models we can modify and re-run the pipeline to add an embedding step and a vector store as a destination. \n",
    "\n",
    "The Unstructured processing pipeline can be assembled from a number of configurations: "
   ],
   "id": "b34afc5988364d8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:05:17.076487Z",
     "start_time": "2024-08-19T13:05:16.031165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pipeline.from_configs(\n",
    "    context=ProcessorConfig(\n",
    "        verbose=True,\n",
    "        tqdm=True,\n",
    "        num_processes=5\n",
    "    ),\n",
    "    indexer_config=LocalIndexerConfig(input_path=\"PDFS\"),\n",
    "    downloader_config=LocalDownloaderConfig(),\n",
    "    source_connection_config=LocalConnectionConfig(),\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=True,\n",
    "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
    "        strategy=\"fast\", # for complex image-based PDFs replace this with \"hi_res\"\n",
    "        additional_partition_args={\n",
    "                \"split_pdf_page\": True,\n",
    "                \"split_pdf_allow_failed\": True,\n",
    "                \"split_pdf_concurrency_level\": 15\n",
    "            }\n",
    "        ),\n",
    "    chunker_config=ChunkerConfig(\n",
    "        chunking_strategy=\"by_title\",\n",
    "        chunk_max_characters=1500,\n",
    "        chunk_overlap = 150,\n",
    "        ),\n",
    "    uploader_config=LocalUploaderConfig(output_dir=\"local-ingest-output\")\n",
    ").run()"
   ],
   "id": "35ddea0bf99f8466",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 09:05:16,033 MainProcess INFO     Created index with configs: {\"input_path\": \"PDFS\", \"recursive\": false}, connection configs: {\"access_config\": {}}\n",
      "2024-08-19 09:05:16,033 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-08-19 09:05:16,034 MainProcess INFO     Created partition with configs: {\"strategy\": \"fast\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": {\"split_pdf_page\": true, \"split_pdf_allow_failed\": true, \"split_pdf_concurrency_level\": 15}, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructuredapp.io/general/v0/general\", \"partition_by_api\": true, \"api_key\": \"*******\", \"hi_res_model_name\": null}\n",
      "2024-08-19 09:05:16,034 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": null, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 1500, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": null, \"chunk_overlap\": 150, \"chunk_overlap_all\": null}\n",
      "2024-08-19 09:05:16,035 MainProcess INFO     Created upload with configs: {\"output_dir\": \"local-ingest-output\"}, connection configs: {\"access_config\": {}}\n",
      "2024-08-19 09:05:16,035 MainProcess INFO     Running local pipline: index (LocalIndexer) -> download (LocalDownloader) -> partition (fast) -> chunk (by_title) -> upload (LocalUploader) with configs: {\"reprocess\": false, \"verbose\": true, \"tqdm\": true, \"work_dir\": \"/Users/mk/.cache/unstructured/ingest/pipeline\", \"num_processes\": 5, \"max_connections\": null, \"raise_on_error\": false, \"disable_parallelism\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null, \"re_download\": false, \"uncompress\": false, \"status\": {}, \"semaphore\": null}\n",
      "2024-08-19 09:05:16,080 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"1. Walmart, Inc.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"rel_path\": \"1. Walmart, Inc.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\"}, \"date_created\": \"1722017589.8771877\", \"date_modified\": \"1722017589.8771877\", \"date_processed\": \"1724072716.080215\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 1742387}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-19 09:05:16,082 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"2.Exxon Mobil Corporation.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"rel_path\": \"2.Exxon Mobil Corporation.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\"}, \"date_created\": \"1722017631.1280465\", \"date_modified\": \"1722017631.1280465\", \"date_processed\": \"1724072716.0821362\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 13891749}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-19 09:05:16,083 MainProcess INFO     Calling DownloadStep with 2 docs\n",
      "2024-08-19 09:05:16,083 MainProcess INFO     processing content async\n",
      "2024-08-19 09:05:16,083 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "download:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 09:05:16,087 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\n",
      "2024-08-19 09:05:16,088 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\n",
      "download: 100%|██████████| 2/2 [00:00<00:00, 690.65it/s]\n",
      "2024-08-19 09:05:16,089 MainProcess INFO     DownloadStep [cls] took 0.006210803985595703 seconds\n",
      "2024-08-19 09:05:16,089 MainProcess INFO     Calling PartitionStep with 2 docs\n",
      "2024-08-19 09:05:16,089 MainProcess INFO     processing content async\n",
      "2024-08-19 09:05:16,090 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "partition:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 09:05:16,092 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/8174f7931121.json\n",
      "2024-08-19 09:05:16,093 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/c54e87910a8f.json\n",
      "partition: 100%|██████████| 2/2 [00:00<00:00, 714.78it/s]\n",
      "2024-08-19 09:05:16,094 MainProcess INFO     PartitionStep [cls] took 0.004625082015991211 seconds\n",
      "2024-08-19 09:05:16,094 MainProcess INFO     Calling ChunkStep with 2 docs\n",
      "2024-08-19 09:05:16,094 MainProcess INFO     processing content across processes\n",
      "chunk:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 09:05:17,054 SpawnPoolWorker-3 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bec3cbee1521.json\n",
      "chunk:  50%|█████     | 1/2 [00:00<00:00,  1.07it/s]2024-08-19 09:05:17,056 SpawnPoolWorker-3 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/3c703e122845.json\n",
      "chunk: 100%|██████████| 2/2 [00:00<00:00,  2.13it/s]\n",
      "2024-08-19 09:05:17,063 MainProcess INFO     ChunkStep [cls] took 0.9686160087585449 seconds\n",
      "2024-08-19 09:05:17,063 MainProcess INFO     Calling UploadStep with 2 docs\n",
      "2024-08-19 09:05:17,066 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bec3cbee1521.json to /Users/mk/Code/eval_data/eval_data/local-ingest-output/1. Walmart, Inc.pdf.json\n",
      "2024-08-19 09:05:17,067 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/chunk/3c703e122845.json to /Users/mk/Code/eval_data/eval_data/local-ingest-output/2.Exxon Mobil Corporation.pdf.json\n",
      "2024-08-19 09:05:17,069 MainProcess INFO     UploadStep [cls] took 0.005360126495361328 seconds\n",
      "2024-08-19 09:05:17,069 MainProcess INFO     Finished ingest process in 1.0337378978729248s\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `ProcessorConfig` describes general behavior such as logs verbosity, number of processes, etc.\n",
    "* `LocalIndexerConfig`, `LocalDownloaderConfig`, and `LocalConnectionConfig` control data ingestion from a local source, you only need to provide a path to your local directory with PDFs here.\n",
    "* `PartitionerConfig`: use it to supply your credentials for the Unstructured Serverless API, and customize the partitioning behavior, e.g. what partitioning strategy to use, whether to exclude some types of metadata, etc. In this case, we use `fast` strategy to partition the files, as the PDFs are not complex and contain text only.\n",
    "* `ChunkerConfig`: after partitioning we will chunk the documents into meaningful sized chunks that are not exceeding the input size of all the embedding models we'll be evaluating.\n",
    "* `LocalUploaderConfig`: specify a local directory to load the processed files into.   "
   ],
   "id": "1224844a4b995677"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create an evaluation dataset",
   "id": "c7eddee18a6d0294"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have preprocessed the documents into chunks, let's build a synthetic evaluation dataset. To load all the processed files from the output directory, we can use the `elements_from_json` function for each JSON file:",
   "id": "44cf59e3058987f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:47:28.727517Z",
     "start_time": "2024-08-20T14:47:28.725161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_processed_files(directory_path):\n",
    "    \"\"\"\n",
    "    Reads all preprocessed data from JSON files in the given directory and returns elements as a list\n",
    "\n",
    "    Args:\n",
    "        :param directory_path: The path to the directory containing JSON files.\n",
    "    \"\"\"\n",
    "    elements = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                elements.extend(elements_to_dicts(elements_from_json(filename=file_path))) \n",
    "            except IOError:\n",
    "                print(f\"Error: Could not read file {filename}.\")\n",
    "    \n",
    "    return elements"
   ],
   "id": "6f0d1c42f564ca25",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:47:32.797486Z",
     "start_time": "2024-08-20T14:47:31.202038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "elements = load_processed_files(\"local-ingest-output\")\n",
    "\n",
    "len(elements)"
   ],
   "id": "51268dc2bca7f007",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add a helper function that will parse LLM's responses into a dictionary, and add `context` (chunk content) and `chunk_id` of the chunk the question is based on, so that we could later see whether we retrieve the chunk or not:",
   "id": "912c5440ec28089e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:47:36.037691Z",
     "start_time": "2024-08-20T14:47:36.033895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_qa_string_to_dict(input_string, chunk_id, chunk_text):\n",
    "    \"\"\"\n",
    "    Converts a string response from an LLM to a Python dictionary with question-answer-context entries.\n",
    "    \n",
    "    Args: \n",
    "        :param input_string: The LLM's response string.\n",
    "        :param chunk_id: Chunk id for the chunk the questions were generated from.\n",
    "        :param chunk_text: Original text of the chunk the questions were generated from.        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.loads(input_string)\n",
    "        questions = result[\"questions\"]\n",
    "        for question in questions:\n",
    "            question['id'] = chunk_id\n",
    "            question['context'] = chunk_text\n",
    "        return questions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return []\n"
   ],
   "id": "496514596d07c6de",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the synthetic evaluation dataset, we'll go over the chunks, and for each chunk we'll prompt the local `llama3.1:8b` model to generate two question/answer pairs.  ",
   "id": "b3550b0d8a0b0b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:55:54.678600Z",
     "start_time": "2024-08-20T14:55:54.674113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_chunk_qa_pairs(element):\n",
    "    \"\"\"\n",
    "    Uses a local LLM to generate two question-answer pairs for a chunk, then \n",
    "    parses the string response to a Python dictionary.\n",
    "    \n",
    "    Args: \n",
    "        element: document element from a json file containing a single chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are an assistant specialized in RAG tasks. \\n\n",
    "    The task is the following: given a document chunk, you will have to\n",
    "    generate questions that can be asked by a user to retrieve information from\n",
    "    a large documentary corpus. \\n\n",
    "    The question should be relevant to the chunk, and should not be too specific\n",
    "    or too general. The question should be about the subject of the chunk, and\n",
    "    the answer needs to be found in the chunk. \\n\n",
    "\n",
    "    Remember that the question is asked by a user to get some information from a\n",
    "    large documentary corpus. \\n\n",
    "\n",
    "    Generate a question that could be asked by a user without knowing the existence and the content of the corpus. \\n\n",
    "    Also generate the answer to the question, which should be found in the\n",
    "    document chunk.  \\n\n",
    "    Generate TWO pairs of questions and answers per chunk in a\n",
    "    dictionary with the following format, your answer should ONLY contain this dictionary, NOTHING ELSE: \\n\n",
    "    {\n",
    "        \"questions\": [\n",
    "            {\n",
    "                \"question\": \"XXXXXX\",\n",
    "                \"answer\": \"YYYYYY\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"XXXXXX\",\n",
    "                \"answer\": \"YYYYYY\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    where XXXXXX is the question, YYYYYY is the corresponding answers that could be as long as needed. \\n\n",
    "    Note: If there are no questions to ask about the chunk, return an empty list.\n",
    "    Focus on making relevant questions concerning the page. \\n\n",
    "    Here is the chunk: \\n\n",
    "\"\"\"    \n",
    "\n",
    "    response = ollama.generate('llama3.1:8b', prompt + element['text'])\n",
    "    return convert_qa_string_to_dict(response['response'], element['element_id'], element['text'])\n",
    "\n",
    "    # replace with the following if you want to switch to Claude3.5-sonnet\n",
    "    # client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "    # response = client.messages.create(\n",
    "    #     model=\"claude-3-5-sonnet-20240620\",\n",
    "    #     max_tokens=1024,\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"user\", \"content\": prompt + element['text']}\n",
    "    #     ]\n",
    "    # )\n",
    "    # return convert_qa_string_to_dict(response.content[0].text, element['element_id'], element['text'])\n",
    "        "
   ],
   "id": "c2b5045385f88315",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:47:44.681793Z",
     "start_time": "2024-08-20T14:47:44.679185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_qa_pairs_dataset(elements):\n",
    "    \"\"\"\n",
    "    Creates a dataset of question-answer-context pairs from a dictionary with elements.\n",
    "    \n",
    "    Args: \n",
    "        :param elements: document element from a json file containing a single chunk\n",
    "    \"\"\"    \n",
    "    \n",
    "    dataset = []\n",
    "    for el in elements:        \n",
    "        dataset.extend(generate_chunk_qa_pairs(el))        \n",
    "    return dataset"
   ],
   "id": "507df1f41e110d3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, let's generate the dataset. \n",
    "Running the following cell can take a long time depending on your hardware, model you use, how large your documents are and how many of them you have. You may also see a few JSON parsing errors, that's ok, that means that some LLM responses were not a correct JSON. In our experiments, there was a negligible amount of them."
   ],
   "id": "66c9df7a12fdb625"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "eval_dataset = generate_qa_pairs_dataset(elements)",
   "id": "c38c11c82f693285",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's save the dataset as a CSV file locally.",
   "id": "505a4be87f773246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:26:07.955172Z",
     "start_time": "2024-08-19T22:26:07.906098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_dataset_as_csv(dict_list, output_file):\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries with QA pairs as a CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df = df[df['question'].notna()]\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"DataFrame saved to {output_file}\")\n",
    "\n",
    "save_dataset_as_csv(eval_dataset, \"qa_pairs_dataset.csv\")"
   ],
   "id": "92c12c172c980dec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to qa_pairs_dataset.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up retrievers and collect responses to questions",
   "id": "5c850fe4a262e68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have an evaluation dataset, we can set up a retriever with each of the embedding models, and retrieve results for each of the question in the evaluation dataset - `setup_and_query_rag()`does just that. ",
   "id": "976ce0e0fb402ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:17:18.920199Z",
     "start_time": "2024-08-20T14:17:18.916428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_and_query_rag(embedding_model, documents, eval_dataset, output_directory, n_to_retrieve=10):\n",
    "    \n",
    "    elements = load_processed_files(documents)\n",
    "    staged_elements = dict_to_elements(elements)\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for element in staged_elements:\n",
    "        metadata = element.metadata.to_dict()\n",
    "        metadata['element_id'] = element._element_id\n",
    "        del metadata['orig_elements']\n",
    "        documents.append(Document(page_content=element.text, metadata=metadata))\n",
    "\n",
    "    documents = chromautils.filter_complex_metadata(documents)\n",
    "    db = Chroma.from_documents(documents, HuggingFaceEmbeddings(model_name=embedding_model))\n",
    "    retriever =  db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": n_to_retrieve})\n",
    "    \n",
    "    df = pd.read_csv(eval_dataset)\n",
    "    df = df[df['question'].notna()]\n",
    "    questions = df[\"question\"].to_list()\n",
    "    \n",
    "    results = []\n",
    "    for question in questions:\n",
    "        try:\n",
    "            retrieved_documents = retriever.invoke(question)\n",
    "            retrieved_ids = [doc.metadata['element_id'] for doc in retrieved_documents]\n",
    "            results.append({\"question\": question, \"retrieved_ids\": retrieved_ids})\n",
    "        except:\n",
    "            print(f\"Skipped question: {question}\")\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    file_path = os.path.join(output_directory, f\"{embedding_model.replace('/', '@')}-{n_to_retrieve}.csv\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"DataFrame saved to {file_path}\")\n",
    "    db.delete_collection()"
   ],
   "id": "ec98d8630ec0e7d3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:36:15.396798Z",
     "start_time": "2024-08-20T14:31:01.325229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\"BAAI/bge-large-en-v1.5\", \"mukaj/fin-mpnet-base\", \"Snowflake/snowflake-arctic-embed-l\"]\n",
    "\n",
    "for model in models:\n",
    "    setup_and_query_rag(model, \"local-ingest-output\", \"qa_pairs_dataset.csv\", \"retriever_results\")"
   ],
   "id": "3595ac4a9ef6226a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to retriever_results/BAAI@bge-large-en-v1.5-10.csv\n",
      "DataFrame saved to retriever_results/mukaj@fin-mpnet-base-10.csv\n",
      "DataFrame saved to retriever_results/Snowflake@snowflake-arctic-embed-l-10.csv\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate the metrics and compare the results",
   "id": "db8b95cf5bde49ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Once you have the results from each of the retrievers, let's calculate some metrics. \n",
    "In this example, we'll use two metrics: Recall@K, and MRR. \n",
    "\n",
    "Since the evaluation dataset has one relevant chunk per question, the average Recall@K will tell us how often this chunk was retrieved _at all_ in the K retrieved documents. The value of 1 would mean that we retrieved the relevant chunk for every question (without taking into account its position in the list of retrieved chunks), the value of 0 would mean that the relevant chunk was never retrieved for any question.\n",
    "\n",
    "The average MRR (Mean reciprocal rank) will tell us the average position of the relevant chunk in the list of retrieved chunks, e.g. mrr = 1 would mean it was always the first result, mrr = 1/2 would mean it was second, etc.    "
   ],
   "id": "a8741862335da8b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:35:24.265969Z",
     "start_time": "2024-08-19T22:35:24.262863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_retrieval_metrics(evaluation_data: pd.DataFrame, retrieval_results: pd.DataFrame, top_k=10):\n",
    "    eval_list = evaluation_data.to_dict('records')\n",
    "    retrieval_list = retrieval_results.to_dict('records')\n",
    "    recall = []\n",
    "    ranks = []\n",
    "\n",
    "    for item in retrieval_list:\n",
    "        question = item[\"question\"]        \n",
    "        retrieved_ids = eval(item[\"retrieved_ids\"])[:top_k]\n",
    "        \n",
    "        for eval_point in eval_list:\n",
    "            if eval_point['question'] == question:\n",
    "                correct_id = eval_point[\"id\"]\n",
    "\n",
    "        if correct_id in retrieved_ids:\n",
    "            recall.append(1)\n",
    "            rank = retrieved_ids.index(correct_id) + 1\n",
    "            ranks.append(1 / rank)  \n",
    "        else:\n",
    "            recall.append(0)\n",
    "            ranks.append(0)\n",
    "        \n",
    "    # Calculate average metrics\n",
    "    avg_recall = sum(recall) / len(retrieval_list)\n",
    "    mrr = sum(ranks) / len(retrieval_list)\n",
    "    metrics = {\n",
    "        'Recall': avg_recall,\n",
    "        'MRR': mrr, \n",
    "    }\n",
    "        \n",
    "    return metrics"
   ],
   "id": "4c4117ff0872692e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:35:27.268476Z",
     "start_time": "2024-08-19T22:35:26.812130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_dataset = pd.read_csv(\"qa_pairs_dataset.csv\")\n",
    "\n",
    "directory_with_retrieval_results = \"retriever_results\"\n",
    "k = 10\n",
    "all_metrics = dict()\n",
    "\n",
    "for filename in os.listdir(directory_with_retrieval_results):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_with_retrieval_results, filename)\n",
    "        try:\n",
    "            model_name = filename[:-4].rsplit('-', 1)[0].replace('@', '/')\n",
    "            retrieval_results = pd.read_csv(file_path)\n",
    "            all_metrics[model_name] = calculate_retrieval_metrics(eval_dataset, retrieval_results, top_k=k) \n",
    "        except IOError:\n",
    "            print(f\"Error: Could not read file {filename}.\")"
   ],
   "id": "a86c2e927eafa1cb",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:35:28.546271Z",
     "start_time": "2024-08-19T22:35:28.542959Z"
    }
   },
   "cell_type": "code",
   "source": "all_metrics",
   "id": "8ce9f2cca713964a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Snowflake/snowflake-arctic-embed-l': {'Recall': 0.3502610346464167,\n",
       "  'MRR': 0.20764157268666042},\n",
       " 'BAAI/bge-large-en-v1.5': {'Recall': 0.8794494542002848,\n",
       "  'MRR': 0.6415374677002579},\n",
       " 'mukaj/fin-mpnet-base': {'Recall': 0.8239202657807309,\n",
       "  'MRR': 0.5528548074822408}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " \n",
    "* 87.9% of the time the relevant id is retrieved. \n",
    "* On average, the relevant id is approximately between the first and second position (1/1.5 = 0.67, and we got 0.64)  "
   ],
   "id": "ae1c5023520fbfca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:38:53.562072Z",
     "start_time": "2024-08-19T22:38:53.559104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_with_max_recall = max(all_metrics, key=lambda k: all_metrics[k]['Recall'])\n",
    "model_with_max_recall"
   ],
   "id": "5477341944e21cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-large-en-v1.5'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complete the preprocessing pipeline with an embedding and upload steps\n",
    "\n",
    "The results of partitioning and chunking are already cached, so by adding an embedding configuration to the pipeline we the pipeline will pick up at the embedding step, and won't re-process the documents from scratch."
   ],
   "id": "3687dcc7f0453579"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:41:31.027567Z",
     "start_time": "2024-08-19T22:39:10.868311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pipeline.from_configs(\n",
    "    context=ProcessorConfig(\n",
    "        verbose=True,\n",
    "        tqdm=True,\n",
    "        num_processes=20,\n",
    "    ),\n",
    "    indexer_config=LocalIndexerConfig(input_path=\"PDFS\"),\n",
    "    downloader_config=LocalDownloaderConfig(),\n",
    "    source_connection_config=LocalConnectionConfig(),\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=True,\n",
    "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
    "        strategy=\"fast\",\n",
    "        additional_partition_args={\n",
    "                \"split_pdf_page\": True,\n",
    "                \"split_pdf_allow_failed\": True,\n",
    "                \"split_pdf_concurrency_level\": 15\n",
    "            }\n",
    "        ),\n",
    "    chunker_config=ChunkerConfig(\n",
    "        chunking_strategy=\"by_title\",\n",
    "        chunk_max_characters=1500,\n",
    "        chunk_overlap = 150,\n",
    "        ),\n",
    "    embedder_config=EmbedderConfig(\n",
    "        embedding_provider=\"langchain-huggingface\",\n",
    "        embedding_model_name=model_with_max_recall, # use the model with the highest recall\n",
    "    ),\n",
    "    uploader_config=LocalUploaderConfig(output_dir=\"outputs-with-embeddings\") # Changing the output location\n",
    ").run()"
   ],
   "id": "17da3022a3108977",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 18:39:10,872 MainProcess INFO     Created index with configs: {\"input_path\": \"PDFS\", \"recursive\": false}, connection configs: {\"access_config\": {}}\n",
      "2024-08-19 18:39:10,873 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-08-19 18:39:10,874 MainProcess INFO     Created partition with configs: {\"strategy\": \"fast\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": {\"split_pdf_page\": true, \"split_pdf_allow_failed\": true, \"split_pdf_concurrency_level\": 15}, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructuredapp.io/general/v0/general\", \"partition_by_api\": true, \"api_key\": \"*******\", \"hi_res_model_name\": null}\n",
      "2024-08-19 18:39:10,875 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": null, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 1500, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": null, \"chunk_overlap\": 150, \"chunk_overlap_all\": null}\n",
      "2024-08-19 18:39:10,875 MainProcess INFO     Created embed with configs: {\"embedding_provider\": \"langchain-huggingface\", \"embedding_api_key\": null, \"embedding_model_name\": \"BAAI/bge-large-en-v1.5\", \"embedding_aws_access_key_id\": null, \"embedding_aws_secret_access_key\": null, \"embedding_aws_region\": null}\n",
      "2024-08-19 18:39:13,251 MainProcess INFO     Created upload with configs: {\"output_dir\": \"outputs-with-embeddings\"}, connection configs: {\"access_config\": {}}\n",
      "2024-08-19 18:39:13,253 MainProcess INFO     Running local pipline: index (LocalIndexer) -> download (LocalDownloader) -> partition (fast) -> chunk (by_title) -> embed (langchain-huggingface) -> upload (LocalUploader) with configs: {\"reprocess\": false, \"verbose\": true, \"tqdm\": true, \"work_dir\": \"/Users/mk/.cache/unstructured/ingest/pipeline\", \"num_processes\": 20, \"max_connections\": null, \"raise_on_error\": false, \"disable_parallelism\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null, \"re_download\": false, \"uncompress\": false, \"status\": {}, \"semaphore\": null}\n",
      "2024-08-19 18:39:13,304 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"1. Walmart, Inc.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"rel_path\": \"1. Walmart, Inc.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\"}, \"date_created\": \"1722017589.8771877\", \"date_modified\": \"1722017589.8771877\", \"date_processed\": \"1724107153.30389\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 1742387}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-19 18:39:13,306 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"2.Exxon Mobil Corporation.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"rel_path\": \"2.Exxon Mobil Corporation.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\"}, \"date_created\": \"1722017631.1280465\", \"date_modified\": \"1722017631.1280465\", \"date_processed\": \"1724107153.306206\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 13891749}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-19 18:39:13,307 MainProcess INFO     Calling DownloadStep with 2 docs\n",
      "2024-08-19 18:39:13,307 MainProcess INFO     processing content async\n",
      "2024-08-19 18:39:13,308 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "download:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:39:13,312 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\n",
      "2024-08-19 18:39:13,313 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\n",
      "download: 100%|██████████| 2/2 [00:00<00:00, 560.70it/s]\n",
      "2024-08-19 18:39:13,314 MainProcess INFO     DownloadStep [cls] took 0.006888866424560547 seconds\n",
      "2024-08-19 18:39:13,314 MainProcess INFO     Calling PartitionStep with 2 docs\n",
      "2024-08-19 18:39:13,314 MainProcess INFO     processing content async\n",
      "2024-08-19 18:39:13,314 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "partition:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:39:13,317 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/c54e87910a8f.json\n",
      "2024-08-19 18:39:13,318 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/8174f7931121.json\n",
      "partition: 100%|██████████| 2/2 [00:00<00:00, 685.62it/s]\n",
      "2024-08-19 18:39:13,319 MainProcess INFO     PartitionStep [cls] took 0.004873037338256836 seconds\n",
      "2024-08-19 18:39:13,319 MainProcess INFO     Calling ChunkStep with 2 docs\n",
      "2024-08-19 18:39:13,319 MainProcess INFO     processing content across processes\n",
      "chunk:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:39:14,986 SpawnPoolWorker-10 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bec3cbee1521.json\n",
      "chunk:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]2024-08-19 18:39:14,991 SpawnPoolWorker-11 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/3c703e122845.json\n",
      "chunk: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
      "2024-08-19 18:39:15,032 MainProcess INFO     ChunkStep [cls] took 1.7130928039550781 seconds\n",
      "2024-08-19 18:39:15,033 MainProcess INFO     Calling EmbedStep with 2 docs\n",
      "2024-08-19 18:39:15,033 MainProcess INFO     processing content across processes\n",
      "embed:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:41:08,764 SpawnPoolWorker-23 DEBUG    Writing embedded output to: /Users/mk/.cache/unstructured/ingest/pipeline/embed/ceb56fa1fb24.json\n",
      "embed:  50%|█████     | 1/2 [01:53<01:53, 113.34s/it]2024-08-19 18:41:30,401 SpawnPoolWorker-22 DEBUG    Writing embedded output to: /Users/mk/.cache/unstructured/ingest/pipeline/embed/abe3b0334474.json\n",
      "embed: 100%|██████████| 2/2 [02:15<00:00, 67.53s/it] \n",
      "2024-08-19 18:41:31,008 MainProcess INFO     EmbedStep [cls] took 135.9749517440796 seconds\n",
      "2024-08-19 18:41:31,008 MainProcess INFO     Calling UploadStep with 2 docs\n",
      "2024-08-19 18:41:31,011 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/embed/ceb56fa1fb24.json to /Users/mk/Code/eval_data/eval_data/outputs-with-embeddings/2.Exxon Mobil Corporation.pdf.json\n",
      "2024-08-19 18:41:31,015 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/embed/abe3b0334474.json to /Users/mk/Code/eval_data/eval_data/outputs-with-embeddings/1. Walmart, Inc.pdf.json\n",
      "2024-08-19 18:41:31,019 MainProcess INFO     UploadStep [cls] took 0.010811090469360352 seconds\n",
      "2024-08-19 18:41:31,020 MainProcess INFO     Finished ingest process in 137.76718425750732s\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "102ef06008388062"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
