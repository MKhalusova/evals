{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Selecting an embedding model for your custom data",
   "id": "fb98851653cfa29b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In a recent blog post, [\"Understanding embedding models: make an informed choice for your RAG\"](https://unstructured.io/blog/understanding-embedding-models-make-an-informed-choice-for-your-rag), we have explored what you need to know in order to navigate the [Hugging Face MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) effortlessly and select a baseline text embedding model. \n",
    "\n",
    "You are likely to find more than one candidate model that meets your criteria. In this case, you should evaluate the candidates on your own data. Good performance on academic benchmarks is oen thing, but your custom data has its own nuances, domain-specific language, and other unique traits.  \n",
    "\n",
    "In this example we'll pick three embedding models from the MTEB leaderboard, evaluate their retrieval performance on custom data consisting of PDF files. We'll preprocess the PDF files with Unstructured Serverless API, generate a synthetic dataset to evaluate the embedding models, calculate the metrics and pick the best performing model, and use the same preprocessing pipeline to embed the documents and load the results into their destination. "
   ],
   "id": "a11c8e4a60143ecf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install the required dependencies\n",
    "\n",
    "First, let's install the libraries that we will be using: \n",
    "\n",
    "* `unstructured` & `unstructured-ingest` for preprocessing documents. \n",
    "* `python-dotenv` to load the environment variables from a `.env` file \n",
    "* `chromadb` and `langchain` to set up retrievers with different embedding models\n",
    "\n",
    "\n",
    "To use this example, you'll need to get an [Unstructured API key](https://unstructured.io/api-key-hosted). The Unstructured Serverless API comes with a 14-day trial capped at 1000 pages per day. "
   ],
   "id": "4880c94377d80ec7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install -qU \"unstructured[pdf, embed-huggingface]\" unstructured-ingest python-dotenv langchain chromadb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the environment variables",
   "id": "de47b15154d698cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T19:45:05.673792Z",
     "start_time": "2024-08-09T19:45:05.665850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('.env')"
   ],
   "id": "e8b7e875f10eceec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocess PDFs from a source location",
   "id": "356e8798d2b085ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:48:16.852289Z",
     "start_time": "2024-08-14T14:48:16.671057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unstructured_ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured_ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured_ingest.v2.processes.connectors.local import (\n",
    "    LocalIndexerConfig,\n",
    "    LocalDownloaderConfig,\n",
    "    LocalConnectionConfig,\n",
    "    LocalUploaderConfig\n",
    ")\n",
    "from unstructured_ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured_ingest.v2.processes.chunker import ChunkerConfig\n",
    "from unstructured_ingest.v2.processes.embedder import EmbedderConfig\n",
    "\n",
    "from unstructured.staging.base import elements_from_json\n",
    "from unstructured.staging.base import elements_to_dicts\n",
    "\n",
    "import json\n",
    "import ollama\n",
    "import pandas as pd"
   ],
   "id": "b5e20edbe453e280",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T12:49:40.046068Z",
     "start_time": "2024-08-09T12:49:38.055093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pipeline.from_configs(\n",
    "    context=ProcessorConfig(\n",
    "        verbose=True,\n",
    "        tqdm=True,\n",
    "        num_processes=20,\n",
    "    ),\n",
    "    indexer_config=LocalIndexerConfig(input_path=\"PDFS\"),\n",
    "    downloader_config=LocalDownloaderConfig(),\n",
    "    source_connection_config=LocalConnectionConfig(),\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=True,\n",
    "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
    "        strategy=\"fast\",\n",
    "        ),\n",
    "    chunker_config=ChunkerConfig(\n",
    "        chunking_strategy=\"by_title\",\n",
    "        chunk_max_characters=1500,\n",
    "        chunk_overlap = 150,\n",
    "        ),\n",
    "    uploader_config=LocalUploaderConfig(output_dir=\"local-ingest-output\")\n",
    ").run()"
   ],
   "id": "35ddea0bf99f8466",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 08:49:38,058 MainProcess INFO     Created index with configs: {\"input_path\": \"PDFS\", \"recursive\": false}, connection configs: {\"access_config\": {}}\n",
      "2024-08-09 08:49:38,058 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-08-09 08:49:38,059 MainProcess INFO     Created partition with configs: {\"strategy\": \"fast\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": null, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructuredapp.io/general/v0/general\", \"partition_by_api\": true, \"api_key\": \"*******\", \"hi_res_model_name\": null}\n",
      "2024-08-09 08:49:38,060 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": null, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 1500, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": null, \"chunk_overlap\": 150, \"chunk_overlap_all\": null}\n",
      "2024-08-09 08:49:38,060 MainProcess INFO     Created upload with configs: {\"output_dir\": \"local-ingest-output\"}, connection configs: {\"access_config\": {}}\n",
      "2024-08-09 08:49:38,060 MainProcess INFO     Running local pipline: index (LocalIndexer) -> download (LocalDownloader) -> partition (fast) -> chunk (by_title) -> upload (LocalUploader) with configs: {\"reprocess\": false, \"verbose\": true, \"tqdm\": true, \"work_dir\": \"/Users/mk/.cache/unstructured/ingest/pipeline\", \"num_processes\": 20, \"max_connections\": null, \"raise_on_error\": false, \"disable_parallelism\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null, \"re_download\": false, \"uncompress\": false, \"status\": {}, \"semaphore\": null}\n",
      "2024-08-09 08:49:38,112 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"1. Walmart, Inc.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"rel_path\": \"1. Walmart, Inc.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\"}, \"date_created\": \"1722017589.8771877\", \"date_modified\": \"1722017589.8771877\", \"date_processed\": \"1723207778.112122\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 1742387}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-09 08:49:38,114 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"2.Exxon Mobil Corporation.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"rel_path\": \"2.Exxon Mobil Corporation.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\"}, \"date_created\": \"1722017631.1280465\", \"date_modified\": \"1722017631.1280465\", \"date_processed\": \"1723207778.114743\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 13891749}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-09 08:49:38,116 MainProcess INFO     Calling DownloadStep with 2 docs\n",
      "2024-08-09 08:49:38,116 MainProcess INFO     processing content async\n",
      "2024-08-09 08:49:38,116 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "download:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-09 08:49:38,119 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\n",
      "2024-08-09 08:49:38,120 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\n",
      "download: 100%|██████████| 2/2 [00:00<00:00, 647.07it/s]\n",
      "2024-08-09 08:49:38,121 MainProcess INFO     DownloadStep [cls] took 0.00535893440246582 seconds\n",
      "2024-08-09 08:49:38,121 MainProcess INFO     Calling PartitionStep with 2 docs\n",
      "2024-08-09 08:49:38,121 MainProcess INFO     processing content async\n",
      "2024-08-09 08:49:38,121 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "partition:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-09 08:49:38,124 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/197366f70ef6.json\n",
      "2024-08-09 08:49:38,125 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/bdfcfaac20b0.json\n",
      "partition: 100%|██████████| 2/2 [00:00<00:00, 608.80it/s]\n",
      "2024-08-09 08:49:38,126 MainProcess INFO     PartitionStep [cls] took 0.00531005859375 seconds\n",
      "2024-08-09 08:49:38,127 MainProcess INFO     Calling ChunkStep with 2 docs\n",
      "2024-08-09 08:49:38,127 MainProcess INFO     processing content across processes\n",
      "chunk:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-09 08:49:39,901 SpawnPoolWorker-76 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/5b1f3b265425.json\n",
      "chunk:  50%|█████     | 1/2 [00:01<00:01,  1.39s/it]2024-08-09 08:49:39,908 SpawnPoolWorker-76 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bdf778eadaca.json\n",
      "chunk: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n",
      "2024-08-09 08:49:40,015 MainProcess INFO     ChunkStep [cls] took 1.8883850574493408 seconds\n",
      "2024-08-09 08:49:40,016 MainProcess INFO     Calling UploadStep with 2 docs\n",
      "2024-08-09 08:49:40,018 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/chunk/5b1f3b265425.json to /Users/mk/Code/eval_data/eval_data/local-ingest-output/1. Walmart, Inc.pdf.json\n",
      "2024-08-09 08:49:40,033 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bdf778eadaca.json to /Users/mk/Code/eval_data/eval_data/local-ingest-output/2.Exxon Mobil Corporation.pdf.json\n",
      "2024-08-09 08:49:40,038 MainProcess INFO     UploadStep [cls] took 0.022483348846435547 seconds\n",
      "2024-08-09 08:49:40,039 MainProcess INFO     Finished ingest process in 1.9783430099487305s\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create an evaluation dataset",
   "id": "c7eddee18a6d0294"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T19:45:09.724995Z",
     "start_time": "2024-08-09T19:45:09.000671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_processed_files(directory_path:str):\n",
    "    \"\"\"\n",
    "    Reads all preprocessed data from JSON files in the given directory and returns elements as a list\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory containing JSON files.\n",
    "    \"\"\"\n",
    "    elements = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                elements.extend(elements_to_dicts(elements_from_json(filename=file_path))) \n",
    "            except IOError:\n",
    "                print(f\"Error: Could not read file {filename}.\")\n",
    "    \n",
    "    return elements"
   ],
   "id": "6f0d1c42f564ca25",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T19:45:11.971737Z",
     "start_time": "2024-08-09T19:45:10.452852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "elements = load_processed_files(\"local-ingest-output\")\n",
    "\n",
    "len(elements)"
   ],
   "id": "51268dc2bca7f007",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T15:11:58.227882Z",
     "start_time": "2024-08-09T15:11:58.223975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_qa_string_to_dict(input_string, chunk_id, chunk_text):\n",
    "    \"\"\"\n",
    "    Converts a string response from an LLM to a Python dictionary with question-answer-context entries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.loads(input_string)\n",
    "        questions = result[\"questions\"]\n",
    "        for question in questions:\n",
    "            question['id'] = chunk_id\n",
    "            question['context'] = chunk_text\n",
    "        return questions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return []\n"
   ],
   "id": "496514596d07c6de",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T15:12:06.868339Z",
     "start_time": "2024-08-09T15:12:06.864329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_chunk_qa_pairs(element):\n",
    "    \"\"\"\n",
    "    Prompts a local LLM to generate a question-answer pairs for a document chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are an assistant specialized in RAG tasks. \\n\n",
    "    The task is the following: given a document chunk, you will have to\n",
    "    generate questions that can be asked by a user to retrieve information from\n",
    "    a large documentary corpus. \\n\n",
    "    The question should be relevant to the chunk, and should not be too specific\n",
    "    or too general. The question should be about the subject of the chunk, and\n",
    "    the answer needs to be found in the chunk. \\n\n",
    "\n",
    "    Remember that the question is asked by a user to get some information from a\n",
    "    large documentary corpus. \\n\n",
    "\n",
    "    Generate a question that could be asked by a user without knowing the existence and the content of the corpus. \\n\n",
    "    Also generate the answer to the question, which should be found in the\n",
    "    document chunk.  \\n\n",
    "    Generate TWO pairs of questions and answers per chunk in a\n",
    "    dictionary with the following format, your answer should ONLY contain this dictionary, NOTHING ELSE: \\n\n",
    "    {\n",
    "        \"questions\": [\n",
    "            {\n",
    "                \"question\": \"XXXXXX\",\n",
    "                \"answer\": \"YYYYYY\",\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"XXXXXX\",\n",
    "                \"answer\": \"YYYYYY\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    where XXXXXX is the question, YYYYYY is the corresponding answers that could be as long as needed. \\n\n",
    "    Note: If there are no questions to ask about the chunk, return an empty list.\n",
    "    Focus on making relevant questions concerning the page. \\n\n",
    "    Here is the chunk: \\n\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt + element['text']\n",
    "        }\n",
    "    ] \n",
    "    response = ollama.chat(model='llama3.1:8b', messages=messages)\n",
    "    \n",
    "    return convert_qa_string_to_dict(response['message']['content'], element['element_id'], element['text'])    \n",
    "    "
   ],
   "id": "c2b5045385f88315",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T15:19:21.904673Z",
     "start_time": "2024-08-09T15:19:21.901629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_qa_pairs_dataset(elements):\n",
    "    dataset = []\n",
    "    for el in elements: \n",
    "        dataset.extend(generate_chunk_qa_pairs(el))\n",
    "    return dataset"
   ],
   "id": "507df1f41e110d3",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T16:46:12.806723Z",
     "start_time": "2024-08-09T15:19:23.201907Z"
    }
   },
   "cell_type": "code",
   "source": "eval_dataset = generate_qa_pairs_dataset(elements)",
   "id": "c38c11c82f693285",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Expecting ',' delimiter: line 9 column 174 (char 747)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 369)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 169)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 166)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 192)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 158)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 196)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 247)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 271)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 161)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 196)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 332)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 171)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 181)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 183)\n",
      "Error parsing JSON: Expecting value: line 5 column 23 (char 153)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 165)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 173)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 234)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 173)\n",
      "Error parsing JSON: Expecting ',' delimiter: line 5 column 56 (char 207)\n",
      "Error parsing JSON: Expecting ',' delimiter: line 5 column 62 (char 206)\n",
      "Error parsing JSON: Expecting value: line 11 column 5 (char 617)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 204)\n",
      "Error parsing JSON: Expecting ',' delimiter: line 9 column 59 (char 483)\n",
      "Error parsing JSON: Expecting ',' delimiter: line 10 column 3 (char 1129)\n",
      "Error parsing JSON: Expecting value: line 11 column 5 (char 731)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 227)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 288)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 285)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 505)\n",
      "Error parsing JSON: Expecting value: line 11 column 5 (char 719)\n",
      "Error parsing JSON: Expecting ',' delimiter: line 10 column 5 (char 879)\n",
      "Error parsing JSON: Expecting property name enclosed in double quotes: line 6 column 9 (char 173)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T16:54:45.047387Z",
     "start_time": "2024-08-09T16:54:45.000869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_dataset_as_csv(dict_list: list[dict], output_file: str):\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries with QA pairs as a CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df = df[df['question'].notna()]\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"DataFrame saved to {output_file}\")\n",
    "\n",
    "save_dataset_as_csv(eval_dataset, \"qa_pairs_dataset.csv\")"
   ],
   "id": "92c12c172c980dec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to qa_pairs_dataset.csv\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up retrievers and collect responses to questions",
   "id": "5c850fe4a262e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:57:34.852496Z",
     "start_time": "2024-08-12T17:55:47.056566Z"
    }
   },
   "cell_type": "code",
   "source": "!python evaluate_retriever.py  --n_documents_to_retrieve 10 --model_name \"BAAI/bge-large-en-v1.5\"",
   "id": "2133c5f551cb8763",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\r\n",
      "\r\n",
      ">> from langchain.embeddings import HuggingFaceEmbeddings\r\n",
      "\r\n",
      "with new imports of:\r\n",
      "\r\n",
      ">> from langchain_community.embeddings import HuggingFaceEmbeddings\r\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\r\n",
      "  warn_deprecated(\r\n",
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing filter_complex_metadata from langchain.vectorstores is deprecated. Please replace deprecated imports:\r\n",
      "\r\n",
      ">> from langchain.vectorstores import filter_complex_metadata\r\n",
      "\r\n",
      "with new imports of:\r\n",
      "\r\n",
      ">> from langchain_community.vectorstores.utils import filter_complex_metadata\r\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\r\n",
      "  warn_deprecated(\r\n",
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n",
      "  warn_deprecated(\r\n",
      "DataFrame saved to Retriever-BAAI-bge-large-en-v1.5-10_results.csv\r\n"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:18:32.185103Z",
     "start_time": "2024-08-12T18:17:35.564486Z"
    }
   },
   "cell_type": "code",
   "source": "!python evaluate_retriever.py  --n_documents_to_retrieve 10 --model_name \"mukaj/fin-mpnet-base\"",
   "id": "8da72d44e1631cc2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\r\n",
      "\r\n",
      ">> from langchain.embeddings import HuggingFaceEmbeddings\r\n",
      "\r\n",
      "with new imports of:\r\n",
      "\r\n",
      ">> from langchain_community.embeddings import HuggingFaceEmbeddings\r\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\r\n",
      "  warn_deprecated(\r\n",
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing filter_complex_metadata from langchain.vectorstores is deprecated. Please replace deprecated imports:\r\n",
      "\r\n",
      ">> from langchain.vectorstores import filter_complex_metadata\r\n",
      "\r\n",
      "with new imports of:\r\n",
      "\r\n",
      ">> from langchain_community.vectorstores.utils import filter_complex_metadata\r\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\r\n",
      "  warn_deprecated(\r\n",
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n",
      "  warn_deprecated(\r\n",
      "DataFrame saved to Retriever-mukaj-fin-mpnet-base-10_results.csv\r\n"
     ]
    }
   ],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T15:01:04.156292Z",
     "start_time": "2024-08-14T14:59:05.159886Z"
    }
   },
   "cell_type": "code",
   "source": "!python evaluate_retriever.py  --n_documents_to_retrieve 10 --model_name \"Snowflake/snowflake-arctic-embed-l\"",
   "id": "afa74e334dc0e76c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\r\n",
      "\r\n",
      ">> from langchain.embeddings import HuggingFaceEmbeddings\r\n",
      "\r\n",
      "with new imports of:\r\n",
      "\r\n",
      ">> from langchain_community.embeddings import HuggingFaceEmbeddings\r\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\r\n",
      "  warn_deprecated(\r\n",
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain/_api/module_import.py:92: LangChainDeprecationWarning: Importing filter_complex_metadata from langchain.vectorstores is deprecated. Please replace deprecated imports:\r\n",
      "\r\n",
      ">> from langchain.vectorstores import filter_complex_metadata\r\n",
      "\r\n",
      "with new imports of:\r\n",
      "\r\n",
      ">> from langchain_community.vectorstores.utils import filter_complex_metadata\r\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\r\n",
      "  warn_deprecated(\r\n",
      "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n",
      "  warn_deprecated(\r\n",
      "DataFrame saved to Retriever-Snowflake-snowflake-arctic-embed-l-10_results.csv\r\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate the metrics and compare the results",
   "id": "db8b95cf5bde49ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:26:11.576592Z",
     "start_time": "2024-08-12T17:26:11.572286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_retrieval_metrics(evaluation_data: pd.DataFrame, retrieval_results: pd.DataFrame, k = 10):\n",
    "    eval_list = evaluation_data.to_dict('records')\n",
    "    retrieval_list = retrieval_results.to_dict('records')\n",
    "\n",
    "    recall_at_k = []\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for item in retrieval_list:\n",
    "        question = item[\"question\"]\n",
    "        \n",
    "        retrieved_ids = eval(item[\"retrieved_ids\"])[:k]\n",
    "        \n",
    "        for eval_point in eval_list:\n",
    "            if eval_point['question'] == question:\n",
    "                correct_id = eval_point[\"id\"]\n",
    "                continue\n",
    "\n",
    "        if correct_id in retrieved_ids:\n",
    "            rank = retrieved_ids.index(correct_id) + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "            recall_at_k.append(1)  \n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "            recall_at_k.append(0)  \n",
    "        \n",
    "    # Calculate average metrics\n",
    "\n",
    "    # Recall@K: (number of relevant items in K results)/(total number of relevant items)\n",
    "    # Sine we only have 1 relevant id in the eval dataset, the average recall@k will indicate \n",
    "    # how often this id was retrieved _at all_ in the 10 retrieved documents\n",
    "    avg_recall_at_k = sum(recall_at_k) / len(retrieval_list)\n",
    "    # How close to the top, on average, the relevant id was in the retrieved list of ids\n",
    "    mrr = sum(reciprocal_ranks) / len(retrieval_list)\n",
    "    metrics = {\n",
    "        'Recall@K': avg_recall_at_k,\n",
    "        'MRR': mrr, \n",
    "    }\n",
    "        \n",
    "    return metrics"
   ],
   "id": "4c4117ff0872692e",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:31:18.725988Z",
     "start_time": "2024-08-12T17:31:18.606572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_dataset = pd.read_csv(\"qa_pairs_dataset.csv\")\n",
    "r1_results = pd.read_csv(\"r1_results_10.csv\")\n",
    "\n",
    "calculate_retrieval_metrics(eval_dataset, r1_results)"
   ],
   "id": "18b780c7c5a54b92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall@K': 0.8789598108747045, 'MRR': 0.6437673083417755}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:31:21.240435Z",
     "start_time": "2024-08-12T17:31:21.139680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r2_results = pd.read_csv(\"r2_results_10.csv\")\n",
    "calculate_retrieval_metrics(eval_dataset, r2_results)"
   ],
   "id": "cb68e43eec1b0330",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall@K': 0.34278959810874704, 'MRR': 0.2054594919133924}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:31:22.540612Z",
     "start_time": "2024-08-12T17:31:22.439607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "r3_results = pd.read_csv(\"r3_results_10.csv\")\n",
    "calculate_retrieval_metrics(eval_dataset, r3_results)"
   ],
   "id": "6baddc6bec350314",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall@K': 0.8307328605200945, 'MRR': 0.5629132425231709}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interpret the results and pick the best baseline model \n",
    "Retriever_1 shows the best results: \n",
    "\n",
    "* 87.8% of the time the relevant id is retrieved. \n",
    "* On average, the relevant id is somewhere around 5th or 6th place in the list of retrieved documents."
   ],
   "id": "ae1c5023520fbfca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complete the preprocessing pipeline with an embedding and upload steps\n",
    "\n",
    "The results of partitioning and chunking are already cached, so by adding an embedding configuration to the pipeline we the pipeline will pick up at the embedding step, and won't re-process the documents from scratch."
   ],
   "id": "3687dcc7f0453579"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:50:50.290642Z",
     "start_time": "2024-08-14T14:48:23.223159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Pipeline.from_configs(\n",
    "    context=ProcessorConfig(\n",
    "        verbose=True,\n",
    "        tqdm=True,\n",
    "        num_processes=20,\n",
    "    ),\n",
    "    indexer_config=LocalIndexerConfig(input_path=\"PDFS\"),\n",
    "    downloader_config=LocalDownloaderConfig(),\n",
    "    source_connection_config=LocalConnectionConfig(),\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=True,\n",
    "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
    "        strategy=\"fast\",\n",
    "        ),\n",
    "    chunker_config=ChunkerConfig(\n",
    "        chunking_strategy=\"by_title\",\n",
    "        chunk_max_characters=1500,\n",
    "        chunk_overlap = 150,\n",
    "        ),\n",
    "    embedder_config=EmbedderConfig(\n",
    "        embedding_provider=\"langchain-huggingface\",\n",
    "        embedding_model_name=\"BAAI/bge-large-en-v1.5\", # Adding the best embedding model\n",
    "    ),\n",
    "    uploader_config=LocalUploaderConfig(output_dir=\"embedded-outputs\") # Changing the output location\n",
    ").run()"
   ],
   "id": "17da3022a3108977",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 10:48:23,227 MainProcess INFO     Created index with configs: {\"input_path\": \"PDFS\", \"recursive\": false}, connection configs: {\"access_config\": {}}\n",
      "2024-08-14 10:48:23,228 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
      "2024-08-14 10:48:23,229 MainProcess INFO     Created partition with configs: {\"strategy\": \"fast\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": null, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructuredapp.io/general/v0/general\", \"partition_by_api\": true, \"api_key\": \"*******\", \"hi_res_model_name\": null}\n",
      "2024-08-14 10:48:23,230 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": null, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 1500, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": null, \"chunk_overlap\": 150, \"chunk_overlap_all\": null}\n",
      "2024-08-14 10:48:23,230 MainProcess INFO     Created embed with configs: {\"embedding_provider\": \"langchain-huggingface\", \"embedding_api_key\": null, \"embedding_model_name\": \"BAAI/bge-large-en-v1.5\", \"embedding_aws_access_key_id\": null, \"embedding_aws_secret_access_key\": null, \"embedding_aws_region\": null}\n",
      "2024-08-14 10:48:24,050 MainProcess INFO     Created upload with configs: {\"output_dir\": \"embedded-outputs\"}, connection configs: {\"access_config\": {}}\n",
      "2024-08-14 10:48:24,050 MainProcess INFO     Running local pipline: index (LocalIndexer) -> download (LocalDownloader) -> partition (fast) -> chunk (by_title) -> embed (langchain-huggingface) -> upload (LocalUploader) with configs: {\"reprocess\": false, \"verbose\": true, \"tqdm\": true, \"work_dir\": \"/Users/mk/.cache/unstructured/ingest/pipeline\", \"num_processes\": 20, \"max_connections\": null, \"raise_on_error\": false, \"disable_parallelism\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null, \"re_download\": false, \"uncompress\": false, \"status\": {}, \"semaphore\": null}\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-08-14 10:48:24,143 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"1. Walmart, Inc.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"rel_path\": \"1. Walmart, Inc.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\"}, \"date_created\": \"1722017589.8771877\", \"date_modified\": \"1722017589.8771877\", \"date_processed\": \"1723646904.1432111\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 1742387}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-14 10:48:24,145 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"2.Exxon Mobil Corporation.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"rel_path\": \"2.Exxon Mobil Corporation.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\"}, \"date_created\": \"1722017631.1280465\", \"date_modified\": \"1722017631.1280465\", \"date_processed\": \"1723646904.145694\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 13891749}, \"additional_metadata\": {}, \"reprocess\": false}\n",
      "2024-08-14 10:48:24,147 MainProcess INFO     Calling DownloadStep with 2 docs\n",
      "2024-08-14 10:48:24,147 MainProcess INFO     processing content async\n",
      "2024-08-14 10:48:24,147 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "download:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-14 10:48:24,152 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\n",
      "2024-08-14 10:48:24,154 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\n",
      "download: 100%|██████████| 2/2 [00:00<00:00, 386.63it/s]\n",
      "2024-08-14 10:48:24,155 MainProcess INFO     DownloadStep [cls] took 0.008759021759033203 seconds\n",
      "2024-08-14 10:48:24,156 MainProcess INFO     Calling PartitionStep with 2 docs\n",
      "2024-08-14 10:48:24,156 MainProcess INFO     processing content async\n",
      "2024-08-14 10:48:24,156 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "partition:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-14 10:48:24,160 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/bdfcfaac20b0.json\n",
      "2024-08-14 10:48:24,161 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/197366f70ef6.json\n",
      "partition: 100%|██████████| 2/2 [00:00<00:00, 428.56it/s]\n",
      "2024-08-14 10:48:24,163 MainProcess INFO     PartitionStep [cls] took 0.0071370601654052734 seconds\n",
      "2024-08-14 10:48:24,163 MainProcess INFO     Calling ChunkStep with 2 docs\n",
      "2024-08-14 10:48:24,163 MainProcess INFO     processing content across processes\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "chunk:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-14 10:48:25,678 SpawnPoolWorker-4 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/5b1f3b265425.json\n",
      "chunk:  50%|█████     | 1/2 [00:00<00:00,  1.79it/s]2024-08-14 10:48:25,682 SpawnPoolWorker-4 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bdf778eadaca.json\n",
      "chunk: 100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n",
      "2024-08-14 10:48:25,802 MainProcess INFO     ChunkStep [cls] took 1.6395409107208252 seconds\n",
      "2024-08-14 10:48:25,803 MainProcess INFO     Calling EmbedStep with 2 docs\n",
      "2024-08-14 10:48:25,803 MainProcess INFO     processing content across processes\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "embed:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-14 10:50:28,087 SpawnPoolWorker-23 DEBUG    Writing embedded output to: /Users/mk/.cache/unstructured/ingest/pipeline/embed/4654925a1120.json\n",
      "embed:  50%|█████     | 1/2 [02:01<02:01, 121.52s/it]2024-08-14 10:50:49,628 SpawnPoolWorker-24 DEBUG    Writing embedded output to: /Users/mk/.cache/unstructured/ingest/pipeline/embed/60fe10f15838.json\n",
      "embed: 100%|██████████| 2/2 [02:23<00:00, 71.57s/it] \n",
      "2024-08-14 10:50:50,270 MainProcess INFO     EmbedStep [cls] took 144.46688580513 seconds\n",
      "2024-08-14 10:50:50,270 MainProcess INFO     Calling UploadStep with 2 docs\n",
      "2024-08-14 10:50:50,273 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/embed/4654925a1120.json to /Users/mk/Code/eval_data/eval_data/embedded-outputs/2.Exxon Mobil Corporation.pdf.json\n",
      "2024-08-14 10:50:50,277 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/embed/60fe10f15838.json to /Users/mk/Code/eval_data/eval_data/embedded-outputs/1. Walmart, Inc.pdf.json\n",
      "2024-08-14 10:50:50,282 MainProcess INFO     UploadStep [cls] took 0.011287689208984375 seconds\n",
      "2024-08-14 10:50:50,282 MainProcess INFO     Finished ingest process in 146.23182106018066s\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "102ef06008388062"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
